{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "QMkveMaE3Esw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in data"
      ],
      "metadata": {
        "id": "tuQ9GnzA4zGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取训练数据\n",
        "df = pd.read_csv ('train.csv')\n",
        "df['year'] = df['year'].map(lambda x: x-2000)\n",
        "#df = df.iloc[:, 1:]\n",
        "\n",
        "# Read in test data set\n",
        "df1 = pd.read_csv('test.csv')\n",
        "#df1['personid'] = df1['personid'].astype(str)\n",
        "#df1['year'] = df1['year'].map(lambda x: x-2000)\n",
        "#df1 = df1_org.iloc[:, 1:]"
      ],
      "metadata": {
        "id": "Gf3wLQVu3Gxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deal with missing value"
      ],
      "metadata": {
        "id": "yScCjHg_KXIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "miss_list = []\n",
        "for i in range(1205):\n",
        "  miss_list.append(math.ceil(df.iloc[:,i].isnull().sum() / 1000))\n",
        "\n",
        "miss_level, var_count = np.unique(miss_list, return_counts=True)\n",
        "print(miss_level)\n",
        "print(var_count)\n",
        "\n",
        "#plt.bar(left=range(1,19), height=var_count, width=0.4, alpha=0.8, color='red')\n",
        "n, bins, patches = plt.hist(x=miss_list, bins=19, color='#0504aa',\n",
        "                    alpha=0.7, rwidth=0.85)\n"
      ],
      "metadata": {
        "id": "Cy9whXNIKO_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "hiJsrqyJ4v9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values using mode\n",
        "imp_freq = SimpleImputer(strategy=\"most_frequent\")\n",
        "i = 0\n",
        "dele_list = []\n",
        "ave_list = []\n",
        "while i < 1205:\n",
        "  name = 'x'+str(i+1)\n",
        "  if df.iloc[:,i+3].isnull().sum() > 16000:\n",
        "    dele_list.append(name)\n",
        "    i+=1\n",
        "    continue\n",
        "  value = df.iloc[:,i+3].values.reshape(-1,1)\n",
        "  imp = imp_freq.fit_transform(value)\n",
        "  col = df.iloc[:,i+3].dropna()\n",
        "  values, counts = np.unique(col, return_counts=True)\n",
        "  freq_ind = np.argmax(counts)\n",
        "  ave_list.append(values[freq_ind])\n",
        "  df.iloc[:,i+3] = imp\n",
        "  i += 1\n",
        "\n",
        "print(len(ave_list))\n",
        "print(len(dele_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlPJCZTP3JgR",
        "outputId": "76648bf8-1f4d-47fa-e046-b6c6a5bb8ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "962\n",
            "243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values using mean\n",
        "imp_mean = SimpleImputer()\n",
        "i = 0\n",
        "dele_list = []\n",
        "ave_list = []\n",
        "while i < 1205:\n",
        "  name = 'x'+str(i+1)\n",
        "  if df.iloc[:,i+3].isnull().sum() > 16000:\n",
        "    dele_list.append(name)\n",
        "    i+=1\n",
        "    continue\n",
        "  value = df.iloc[:,i+3].values.reshape(-1,1)\n",
        "  imp = imp_mean.fit_transform(value)\n",
        "  ave = np.mean(df.iloc[:,i+3])\n",
        "  ave_list.append(ave)\n",
        "  df.iloc[:,i+3] = imp\n",
        "  i += 1\n",
        "\n",
        "print(len(ave_list))\n",
        "print(len(dele_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBq9hL4W3UQ-",
        "outputId": "194e26ab-93b7-4a9c-e8dc-d452b45f7e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "962\n",
            "243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data drop deleting columns\n",
        "df.drop(columns=dele_list, inplace=True)\n",
        "df1.drop(columns=dele_list, inplace=True)"
      ],
      "metadata": {
        "id": "5XMHy4Jg47qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation for test data set\n",
        "i = 0\n",
        "while i < len(ave_list):\n",
        "  name = 'x'+str(i+1)\n",
        "  df1.iloc[:, i+3].fillna(ave_list[i], inplace=True)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "_lBFxDd15SQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get x, y"
      ],
      "metadata": {
        "id": "HZZ28pS_6q_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.iloc[:,-1]\n",
        "x = df.drop(columns=['uniqueid', 'health'])\n",
        "#x['personid']= x['personid'].map(str)\n",
        "x_columns = x.columns.values.tolist()\n",
        "#print(x_columns)\n",
        "#x"
      ],
      "metadata": {
        "id": "pqImJfXY6qne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = 0\n",
        "test_size = 0.3  \n",
        "  \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=state)"
      ],
      "metadata": {
        "id": "N972YahvGzPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "aqmEh1R76GxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "e7JJ6NBzBika"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tuning\n",
        "\n",
        "scorel = []\n",
        "for i in range(0,400,20):\n",
        "  print(i)\n",
        "  rfc = RandomForestClassifier(n_estimators=i+1, n_jobs=-1,random_state=123)\n",
        "  score = cross_val_score(rfc,x, y, cv=5).mean()\n",
        "  scorel.append(score)\n",
        "\n",
        "\n",
        "#for max_depth in np.arrange()"
      ],
      "metadata": {
        "id": "j49zrqrEQa2o",
        "outputId": "41189170-da2b-41b5-d6c6-3fc0ac1576e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "220\n",
            "240\n",
            "260\n",
            "280\n",
            "300\n",
            "320\n",
            "340\n",
            "360\n",
            "380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scorel)\n",
        "print(max(scorel),(scorel.index(max(scorel))*20)+1) "
      ],
      "metadata": {
        "id": "Fd7pdJ9ygRZI",
        "outputId": "4b0d403d-e075-4639-ee46-3c59e10418b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33210629953778503, 0.42211774753611986, 0.4342460344992782, 0.4385563628073862, 0.44539631879949654, 0.4460283044738051, 0.4459132628282007, 0.4477525745606645, 0.4469478612231682, 0.4482698985373654, 0.44930457952972347, 0.4482700306931903, 0.45137356156644304, 0.451776042131277, 0.4520059932666608, 0.4516608352908915, 0.4536150565131346, 0.45269538412742455, 0.453327617593905, 0.4537299329639578]\n",
            "0.4537299329639578 381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'max_depth':np.arange(1, 20, 1)}\n",
        "\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=100,random_state=90)\n",
        "\n",
        "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
        "GS.fit(x,y)"
      ],
      "metadata": {
        "id": "IGl9nft1iBHR",
        "outputId": "aa745d5c-2d93-4620-8842-eab1ea6df5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=90),\n",
              "             param_grid={'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19])})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GS.best_params_)\n",
        "print(GS.best_score_)"
      ],
      "metadata": {
        "id": "oiJSeGAmiPQ1",
        "outputId": "fb682119-0849-42ae-d765-82f96a57f8d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 17}\n",
            "0.4609136906532358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid={'min_samples_split':np.arange(2, 2+30, 1)}\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=90)\n",
        "\n",
        "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
        "GS.fit(x,y)"
      ],
      "metadata": {
        "id": "2NFv1xUznayr",
        "outputId": "d6a8d5a7-e030-4f18-ead9-b6933f7df5ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=90),\n",
              "             param_grid={'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GS.best_params_)\n",
        "print(GS.best_score_)"
      ],
      "metadata": {
        "id": "lvJLdpcNnpVe",
        "outputId": "cbaab836-1b48-4d64-8bdb-9bb8ae5cf56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'min_samples_split': 26}\n",
            "0.4651096547758323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max feature tuning\n",
        "param_grid = {'max_features':np.arange(5,205,5)}\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=90)\n",
        "\n",
        "GS = GridSearchCV(rfc,param_grid,cv=10)\n",
        "GS.fit(x,y)"
      ],
      "metadata": {
        "id": "7xLgpB4o7zuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GS.best_params_)\n",
        "print(GS.best_score_)"
      ],
      "metadata": {
        "id": "CVptBCgT73r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomforest\n",
        "n_est = 320\n",
        "max_d = 13\n",
        "max_f = 150\n",
        "\n",
        "clf_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_d, \n",
        "                max_features=max_f, bootstrap=True, \n",
        "                oob_score=True, random_state=0)\n",
        "clf_rf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tee9wApS6GMd",
        "outputId": "deed530d-454f-4b4e-f59a-92250d9aa706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=13, max_features=150, n_estimators=300,\n",
              "                       oob_score=True, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model score\n",
        "\n",
        "print(clf_rf.oob_score_)\n",
        "# calculate cross entropy (log-loss) on training data\n",
        "print(log_loss(y_train, clf_rf.predict_proba(x_train)))\n",
        "\n",
        "y_pred = clf_rf.predict(x_test)\n",
        "\n",
        "print(log_loss(y_pred, clf_rf.predict_proba(x_test)))\n",
        "\n",
        "#print(accuray_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "2X0dsZPz7Ueq",
        "outputId": "2561df2c-00f7-4a06-daf4-6b7987639f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4769256035473805\n",
            "0.7265215548028884\n",
            "0.7836939297887454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# produce importance\n",
        "# don't need to run every time\n",
        "print(clf_rf.n_features_in_)\n",
        "feature_dic = {}\n",
        "fea_name = clf_rf.feature_names_in_\n",
        "\n",
        "fea_importance = clf_rf.feature_importances_\n",
        "\n",
        "print(fea_name.shape)\n",
        "print(fea_importance.shape)\n",
        "for i in range(clf_rf.n_features_in_):\n",
        "  feature_dic[fea_name[i]] = fea_importance[i]\n",
        "print(len(feature_dic))\n",
        "print(feature_dic)\n",
        "\n",
        "new_dic = sorted(feature_dic.items(), key=lambda x: x[1], reverse=True)\n",
        "print(new_dic[:20])"
      ],
      "metadata": {
        "id": "-MyNYbrm6P_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf_rf.predict_proba(df1.iloc[:,1:]).T\n",
        "pred = pd.DataFrame(pred)\n",
        "pred = pred.append(df1['uniqueid'])\n",
        "pred = pred.T\n",
        "pred.columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'uniqueid']\n",
        "pred = pred[['uniqueid', 'p1', 'p2', 'p3', 'p4', 'p5']]\n",
        "pred['uniqueid'] = pred['uniqueid'].astype(int)\n",
        "pred.to_csv('result_rf.csv', index=False)"
      ],
      "metadata": {
        "id": "KfeqOqM97FZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "V4u6CM-vE40E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "clf_lr = LogisticRegression(penalty='l1', solver='saga', random_state=0).fit(x_train, y_train)\n",
        "print(clf_lr.score(x,y))\n",
        "\n",
        "print(log_loss(y_train, clf_rf.predict_proba(x_train)))\n",
        "\n",
        "y_pred = clf_rf.predict(x_test)\n",
        "\n",
        "print(log_loss(y_pred, clf_rf.predict_proba(x_test)))"
      ],
      "metadata": {
        "id": "SyUBNqijE6rh",
        "outputId": "5f4100f6-fa89-4991-dcbe-5575a3fc11ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40464421197838835\n",
            "0.7265215548028884\n",
            "0.7836939297887454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Lasso feature selection\n",
        "print(clf_lr.n_features_in_)\n",
        "feature_dic = {}\n",
        "fea_name = clf_lr.feature_names_in_\n",
        "\n",
        "fea_importance = np.mean(np.abs(clf_lr.coef_), axis=0)\n",
        "\n",
        "print(fea_name.shape)\n",
        "print(fea_importance.shape)\n",
        "for i in range(clf_lr.n_features_in_):\n",
        "  feature_dic[fea_name[i]] = fea_importance[i]\n",
        "print(len(feature_dic))\n",
        "print(feature_dic)\n",
        "\n",
        "new_dic = sorted(feature_dic.items(), key=lambda x: x[1], reverse=True)\n",
        "print(new_dic[:20])"
      ],
      "metadata": {
        "id": "LVvlaZNCFHiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lr = clf_lr.predict_proba(df1.iloc[:,1:]).T\n",
        "pred_lr = pd.DataFrame(pred_lr)\n",
        "pred_lr = pred_lr.append(df1_org['uniqueid'])\n",
        "pred_lr = pred_lr.T\n",
        "pred_lr.columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'uniqueid']\n",
        "pred_lr = pred_lr[['uniqueid', 'p1', 'p2', 'p3', 'p4', 'p5']]\n",
        "pred_lr['uniqueid'] = pred_lr['uniqueid'].astype(int)\n",
        "pred_lr\n",
        "pred_lr.to_csv('result_lr.csv', index=False)"
      ],
      "metadata": {
        "id": "bjUyVo9aE-be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient boosting"
      ],
      "metadata": {
        "id": "OZETm2hmGZeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization"
      ],
      "metadata": {
        "id": "0o-W4-tGLF82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = pd.DataFrame(scaler.transform(x_train))\n",
        "x_train.columns = x_columns\n",
        "scaler.fit(x_test)\n",
        "x_test = pd.DataFrame(scaler.transform(x_test))\n",
        "x_test.columns = x_columns\n",
        "x_train\n",
        "\n",
        "df1_columns = df1.columns.values.tolist()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df1.iloc[:, 3:])\n",
        "df1.iloc[:, 3:] = pd.DataFrame(scaler.transform(df1.iloc[:, 3:]))\n",
        "df1.columns = df1_columns\n",
        "df1"
      ],
      "metadata": {
        "id": "PhatxU0zLHRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting\n",
        "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "estimator_list = [20, 50, 100, 150, 200]\n",
        "max_features_list = [10, 50, 100, 125, 150, 175]\n",
        "max_depth_list = [5, 8, 11, 13]\n",
        "min_samples_split = 10\n",
        "min_samples_leaf = 5\n",
        "\n",
        "training_acc = []\n",
        "testing_acc = []\n",
        "\n",
        "i = 0\n",
        "\n",
        "for learning_rate in lr_list:\n",
        "    for n_estimator in estimator_list:\n",
        "        for max_feature in max_features_list:\n",
        "            for max_depth in max_depth_list:\n",
        "    \n",
        "                gb_clf = GradientBoostingClassifier(min_samples_split=10, min_samples_leaf=5, n_estimators=n_estimator, learning_rate=learning_rate, max_features=max_feature, max_depth=max_depth, random_state=0)\n",
        "                gb_clf.fit(x_train, y_train)\n",
        "\n",
        "                print(\"Model-{} \".format(i), \"Learning rate: \", learning_rate, \"n_estimator: \", n_estimator, \"max_feature: \", max_feature, \"max_depth: \", max_depth)\n",
        "                print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(x, y)))\n",
        "                print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(x_test, y_test)))\n",
        "                print('*******************************************************************************')\n",
        "\n",
        "                training_acc.append(gb_clf.score(x_train, y_train))\n",
        "                testing_acc.append(gb_clf.score(x_test, y_test))\n",
        "                i += 1"
      ],
      "metadata": {
        "id": "6fhA9f2mGYKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_acc)"
      ],
      "metadata": {
        "id": "ujwl2YRGJgC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.5, max_features=150, max_depth=13, random_state=0)\n",
        "gb_clf.fit(x_train, y_train)\n",
        "\n",
        "pred = gb_clf.predict_proba(df1.iloc[:,1:]).T\n",
        "pred = pd.DataFrame(pred)\n",
        "pred = pred.append(df1['uniqueid'])\n",
        "pred = pred.T\n",
        "pred.columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'uniqueid']\n",
        "pred = pred[['uniqueid', 'p1', 'p2', 'p3', 'p4', 'p5']]\n",
        "pred['uniqueid'] = pred['uniqueid'].astype(int)\n",
        "pred\n",
        "pred.to_csv('result_gb.csv', index=False)"
      ],
      "metadata": {
        "id": "mc65pg6dJka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "def model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_dim=964, init='normal', activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(5, input_dim=10, init='normal', activation='relu'))\n",
        "  model.add(Dense(5, input_dim=5, init='normal', activation='softmax'))\n",
        "  # Compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "VCJHzHvAdT5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = KerasClassifier(build_fn=model, nb_epoch=200, batch_size=5, verbose=0)"
      ],
      "metadata": {
        "id": "WaDiKC7bepvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n=len(x), n_folds=10, shuffle=True, random_state=seed)"
      ],
      "metadata": {
        "id": "6oqBpLC-eijX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = cross_val_score(estimator, x, y, cv=kfold)\n",
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "GV1a7bZXel0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}